---
title: 'Predictive Policing'
author: "Ziyi Guo"
date: "04/02/2024"
output: html_document
---



```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, warning=FALSE)

install.packages(RSocrata)

library(tidyverse)
library(sf)
library(RSocrata)
library(viridis)
library(spatstat)
library(raster)
library(spdep)
library(FNN)
library(grid)
library(gridExtra)
library(knitr)
library(kableExtra)
library(tidycensus)
library(classInt)   # for KDE and ML risk class intervals
# functions
root.dir = "https://raw.githubusercontent.com/urbanSpatial/Public-Policy-Analytics-Landing/master/DATA/"
source("https://raw.githubusercontent.com/urbanSpatial/Public-Policy-Analytics-Landing/master/functions.r")

```

## Read in Data from Chicago


```{r read_data}

## Potential Predictors

#SHOTSPOTTER SINCE 2017
ShotSpotter <- 
  st_read("https://data.cityofchicago.org/resource/3h7q-7mdb.geojson") %>%
  st_transform('ESRI:102271')

#POLICE STATION 2016
Station_data <- 
  read.csv(file.path("C:/Users/25077/Desktop/MUSA 508_PPA/Chicago_Crime_Predict_Model/Chicago_Crime_Predict_Model/Police_Stations.csv"))
PolStation <- 
  Station_data %>% 
  st_as_sf(coords = c("LONGITUDE", "LATITUDE"), crs = 4326, agr = "constant") %>%
  st_transform('ESRI:102271')%>%
  dplyr::select(DISTRICT, ZIP)%>%
  mutate(Legend = "Pol_Station")

# ABANDONED VEHICLES 2017
abandonCars <- 
  read.socrata("https://data.cityofchicago.org/Service-Requests/311-Service-Requests-Abandoned-Vehicles/3c9v-pnva") %>%
    # Extract the year from the creation date and filter for the year 2017
    mutate(year = substr(creation_date, 1, 4)) %>% filter(year == "2017") %>%
    dplyr::select(Y = latitude, X = longitude) %>%
    na.omit() %>%
    st_as_sf(coords = c("X", "Y"), crs = 4326, agr = "constant") %>%
    st_transform('ESRI:102271') %>%
    mutate(Legend = "Abandoned_Cars")

#SANITATION 2017
Sani_data <- 
  read.csv(file.path("C:/Users/25077/Desktop/MUSA 508_PPA/Chicago_Crime_Predict_Model/Chicago_Crime_Predict_Model/Sanitation_Code_Complaints.csv"))
Sanitation <- 
  Sani_data %>% 
  dplyr::select(Latitude, Longitude, Creation.Date ) %>%
  filter(!is.na(Longitude) & !is.na(Latitude)) %>%
  filter(grepl("2017", `Creation.Date`)) %>%
  st_as_sf(coords = c("Longitude", "Latitude"), crs = 4326, agr = "constant") %>%
  st_transform('ESRI:102271')%>%
  mutate(Legend = "Sanitation")

## Crime Data

# Read and process Robbery with Weapon data
Rob17 <- 
  read.socrata("https://data.cityofchicago.org/Public-Safety/Crimes-2017/d62x-nvdr") %>% 
  filter(Primary.Type == "ROBBERY" & Description == "ARMED: HANDGUN") %>%  
  mutate(x = gsub("[()]", "", Location)) %>% 
  separate(x, into = c("Y", "X"), sep = ",") %>%  
  mutate(X = as.numeric(X), Y = as.numeric(Y)) %>%  
  na.omit() %>% 
  st_as_sf(coords = c("X", "Y"), crs = 4326, agr = "constant") %>%  
  st_transform('ESRI:102271') %>%  
  distinct() 

# Read and process police districts data
policeDistricts <- 
  st_read("https://data.cityofchicago.org/api/geospatial/fthy-xz3r?method=export&format=GeoJSON") %>%
  st_transform('ESRI:102271') %>%  
  dplyr::select(District = dist_num)  

# Read and process police beats data
policeBeats <- 
  st_read("https://data.cityofchicago.org/api/geospatial/aerh-rz74?method=export&format=GeoJSON") %>%
  st_transform('ESRI:102271') %>% 
  dplyr::select(District = beat_num) 

## Boundary Data

# Read and process Chicago boundary data
chicagoBoundary <- 
  st_read(file.path(root.dir, "/Chapter5/chicagoBoundary.geojson")) %>%  
  st_transform('ESRI:102271') 

# Read neighborhood boundaries for Chicago and transform to match fishnet CRS
neighborhoods <- 
  st_read("https://raw.githubusercontent.com/blackmad/neighborhoods/master/chicago.geojson") %>%
  st_transform('ESRI:102271')

```

## 1.Visualizing crime point data


```{r}
  
# Robberies overlaid on Chicago boundary
  ggplot() + 
    geom_sf(data = chicagoBoundary, fill = "lightgrey", colour = NA) +  
    geom_sf(data = Rob17, colour = "indianred", size = 0.1, show.legend = "point") +  
    labs(title = "Robberies, Chicago - 2017") +  # Set plot title
    theme_void()

```

*Selection Bias* 

Reporting Bias: Not all crimes are reported to the police. The likelihood of reporting can vary by type of crime, location, and demographic factors. For example, certain neighborhoods may have higher or lower rates of reporting due to trust in law enforcement, or the nature of the crime may be such that victims are reluctant to report it.

Recording Bias: Even when crimes are reported, there may be inconsistencies in how they are recorded. Different jurisdictions might classify crimes differently, and there might be human error in recording the details.

Law Enforcement Practices: Policing practices can influence where and how crimes are recorded. For example, areas with heavier police patrols may have higher recorded crime rates not because they are inherently more dangerous, but because there is a higher chance of crimes being observed and recorded.

Systemic Issues: Social, economic, and political factors can lead to certain groups being over-policed and over-represented in crime statistics. This can create a feedback loop where the perceived high crime rates in these communities lead to more intense policing, which in turn leads to more crime reports.

Technological Changes: Advancements in technology can change the way crimes are reported and recorded. For example, the introduction of online reporting systems might increase the reporting rate for certain types of crimes.

Media Influence: Media reporting on crime can influence public perception and reporting behavior. High-profile crimes might lead to increased reporting of similar crimes due to heightened awareness.


## 2. Crime joined to fishnet grid


```{r fishnet}

# Create fishnet
fishnet <- 
st_make_grid(chicagoBoundary,
               cellsize = 500, # 500-coordinate system
               square = TRUE) %>%
  .[chicagoBoundary] %>%            # fast way to select intersecting polygons
  st_sf() %>%   mutate(uniqueID = 1:n())


# Based on fishnet to aggregate crime
crime_net <- 
  dplyr::select(Rob17) %>% 
  mutate(countRob = 1) %>% 
  aggregate(., fishnet, sum) %>%
  mutate(countRob = replace_na(countRob, 0),
         uniqueID = 1:n(),
         cvID = sample(round(nrow(fishnet) / 24), 
                       size=nrow(fishnet), replace = TRUE))

ggplot() +
  geom_sf(data = crime_net, aes(fill = countRob), color = NA) +
  scale_fill_viridis("Count of Robberies") +
  labs(title = "Count of Robberies for the fishnet") +
  theme_void()

```



# Modeling Spatial Features

## Abandoned Cars

1. Abandoned Cars Count and knn

```{r jointofishnet}

# COUNT
# Join the abandoned cars data with the fishnet grid based on spatial intersection
# a new spatial grid - abandoned cars

vars_net_Car <- abandonCars %>%  
  st_join(fishnet, join = st_within) %>%  
  st_drop_geometry() %>% 
  group_by(uniqueID, Legend) %>%  
  summarize(count = n()) %>% 
  left_join(fishnet, ., by = "uniqueID") %>%  
  spread(Legend, count, fill = 0) %>%  
  dplyr::select(-`<NA>`) %>%  #
  ungroup() 

# KNN
# Convenience aliases to reduce the length of function names
st_c    <- st_coordinates 
st_coid <- st_centroid    

# vars_net_Car include the count of the abandoned car and the 5 nn feature
vars_net_Car <- vars_net_Car %>%  
    mutate(Abandoned_Cars.nn = nn_function(  
        st_c(st_coid(vars_net_Car)),  # Calculate centroids of fishnet grid cells
        st_c(abandonCars),         # Get coordinates of abandoned cars
        k = 5                      # Number of nearest neighbors to find
    ))
# Visualize the nn
vars_net_Car.long.nn <- 
  dplyr::select(vars_net_Car, ends_with(".nn")) %>%
    gather(Variable, value, -geometry)

#join the abandoned cars (counts,k=5 nn) to the crime fishnet
final_net1 <-
  left_join(crime_net, st_drop_geometry(vars_net_Car), by="uniqueID") 


# LOCAL MORAN's I

## used to generate the weights of neighborhoods
## prerequisite for Local Moran

final_net1.nb <- poly2nb(as_Spatial(final_net1), queen=TRUE) # identify neighborhood units
final_net1.weights <- nb2listw(final_net1.nb, style="W", zero.policy=TRUE) #reflect the degree of spatial relationship or connection between them
print(final_net1.weights, zero.policy=TRUE)


# calculates the local Moran's I statistic for the Abandoned_Cars variable within the final_net data frame
local_morans <- localmoran(final_net1$Abandoned_Cars, final_net1.weights, zero.policy=TRUE) %>% 
  as.data.frame()

# join local Moran's I results to fishnet
final_net1.localMorans <- 
  cbind(local_morans, as.data.frame(final_net1)) %>% 
  st_sf() %>%
  dplyr::select(Abandoned_Cars_Count = Abandoned_Cars, 
                Local_Morans_I = Ii, 
                P_Value = `Pr(z != E(Ii))`) %>%
  mutate(Significant_Hotspots = ifelse(P_Value <= 0.001, 1, 0)) %>%
  gather(Variable, Value, -geometry)
  
```


3.  Visualize local Moran's I results

*Identify Hot Spots Area*
using Local Moran's I, P value to identify the significance
Significant_Hotspots more directly shows the hot spots area, where abandoned cars concentrated.

```{r fig.width=10, fig.height=4}

vars <- unique(final_net1.localMorans$Variable)
varList <- list()

for(i in vars){
  varList[[i]] <- 
    ggplot() +
      geom_sf(data = filter(final_net1.localMorans, Variable == i), 
              aes(fill = Value), colour=NA) +
      scale_fill_viridis(name="") +
      labs(title=i) +
      theme_void() + theme(legend.position="bottom")}

do.call(grid.arrange,c(varList, ncol = 4, top = "Local Morans I statistics, Abandoned Cars"))
```


4. Distance to Hot spot

```{r}
# generates warning from NN

final_net1 <- final_net1 %>% 
  mutate(abandoned.isSig = 
           ifelse(local_morans[,5] <= 0.001, 1, 0)) %>% # identify hotspot area
  mutate(abandoned.isSig.dist = 
           nn_function(st_c(st_coid(final_net1)),
                       st_c(st_coid(filter(final_net1, 
                                           abandoned.isSig == 1))), # dist to the hotspot
                       k = 3))


ggplot() +
      geom_sf(data = final_net1, aes(fill=abandoned.isSig.dist), colour=NA) +
      scale_fill_viridis(name="NN Distance") +
      labs(title="Abandoned Car NN Distance") +
      theme_void()
```


## Sanitation

1. Sanitation Count and knn

```{r jointofishnet}

# Join the Sanitation Count data with the fishnet grid based on spatial intersection
# a new spatial grid - Sanitation Count

vars_net_Sani <- Sanitation %>%  
  st_join(fishnet, join = st_within) %>%  
  st_drop_geometry() %>% 
  group_by(uniqueID, Legend) %>%  
  summarize(count = n()) %>% 
  left_join(fishnet, ., by = "uniqueID") %>%  
  spread(Legend, count, fill = 0) %>%  
  dplyr::select(-`<NA>`) %>%  #
  ungroup() 

# KNN
# Convenience aliases to reduce the length of function names
st_c    <- st_coordinates 
st_coid <- st_centroid    


# vars_net_Sani include the count of the abandoned car and the 5 nn feature
centroids <- st_centroid(vars_net_Sani)

# Extract coordinates as matrices
centroids_coords <- st_coordinates(centroids)
sanitation_coords <- st_coordinates(st_geometry(Sanitation))

# Pass coordinates to the nn_function
vars_net_Sani <- vars_net_Sani %>%
  mutate(Sanitation.nn = nn_function(centroids_coords, sanitation_coords, k = 5))

# Visualize the nn
vars_net_Sani.long.nn <- 
  dplyr::select(vars_net_Sani, ends_with(".nn")) %>%
    gather(Variable, value, -geometry)


#join the abandoned cars (counts,k=5 nn) to the initial
final_net2 <-
  left_join(final_net1, st_drop_geometry(vars_net_Sani), by="uniqueID") 


# LOCAL MORAN's I

## used to generate the weights of neighborhoods
## prerequisite for Local Moran

final_net2.nb <- poly2nb(as_Spatial(final_net2), queen=TRUE) # identify neighborhood units
final_net2.weights <- nb2listw(final_net2.nb, style="W", zero.policy=TRUE) 
print(final_net2.weights, zero.policy=TRUE)


# Local Moran
# calculates the local Moran's I statistic for the Sanitation variable within the final_net data frame
local_morans <- localmoran(final_net2$Sanitation, final_net2.weights, zero.policy=TRUE) %>% 
  as.data.frame()

# join local Moran's I results to fishnet
final_net2.localMorans <- 
  cbind(local_morans, as.data.frame(final_net2)) %>% 
  st_sf() %>%
  dplyr::select(SaniCount = Sanitation, 
                Local_Morans_I = Ii, 
                P_Value = `Pr(z != E(Ii))`) %>%
  mutate(Significant_Hotspots = ifelse(P_Value <= 0.001, 1, 0)) %>%
  gather(Variable, Value, -geometry)
  
```


3.  Visualize local Moran's I results

*Identify Hot Spots Area*
using Local Moran's I, P value to identify the significance
Significant_Hotspots more directly shows the hot spots area, where abandoned cars concentrated.

```{r fig.width=10, fig.height=4}

vars <- unique(final_net2.localMorans$Variable)
varList <- list()

for(i in vars){
  varList[[i]] <- 
    ggplot() +
      geom_sf(data = filter(final_net2.localMorans, Variable == i), 
              aes(fill = Value), colour=NA) +
      scale_fill_viridis(name="") +
      labs(title=i) +
      theme_void() + theme(legend.position="bottom")}

do.call(grid.arrange,c(varList, ncol = 4, top = "Local Morans I statistics, Sanitation Status"))
```


4. Distance to Hot spot

```{r}
# generates warning from NN
final_net2 <- final_net2 %>% 
  mutate(Sani.isSig = 
           ifelse(local_morans[,5] <= 0.001, 1, 0)) %>% # identify hotspot area
  mutate(Sani.isSig.dist = 
           nn_function(st_c(st_coid(final_net2)),
                       st_c(st_coid(filter(final_net2, 
                                           Sani.isSig == 1))), # dist to the hotspot
                       k = 3))


ggplot() +
      geom_sf(data = final_net2, aes(fill=Sani.isSig.dist), colour=NA) +
      scale_fill_viridis(name="NN Distance") +
      labs(title="Sanitation NN Distance") +
      theme_void()
```
## Police Station

1. police station Count and knn

```{r jointofishnet}

# Join the Sanitation Count data with the fishnet grid based on spatial intersection
# a new spatial grid - Sanitation Count


vars_net_station <- PolStation %>%  
  st_join(fishnet, join = st_within) %>%  
  st_drop_geometry() %>% 
  group_by(uniqueID, Legend) %>%  
  summarize(count = n()) %>% 
  left_join(fishnet, ., by = "uniqueID") %>%  
  spread(Legend, count, fill = 0) %>%  
  dplyr::select(-`<NA>`) %>%  #
  ungroup() 

# KNN
# Convenience aliases to reduce the length of function names
st_c    <- st_coordinates 
st_coid <- st_centroid    


# vars_net_station include the count of the abandoned car and the 5 nn feature
centroids <- st_centroid(vars_net_station)

# Extract coordinates as matrices
centroids_coords <- st_coordinates(centroids)
station_coords <- st_coordinates(st_geometry(PolStation))

# Pass coordinates to the nn_function
vars_net_station <- vars_net_station %>%
  mutate(PolStation.nn = nn_function(centroids_coords, station_coords, k = 5))

# Visualize the nn
vars_net_station.long.nn <- 
  dplyr::select(vars_net_station, ends_with(".nn")) %>%
    gather(Variable, value, -geometry)


#join the abandoned cars (counts,k=5 nn) to the initial
final_net3 <-
  left_join(final_net2, st_drop_geometry(vars_net_station), by="uniqueID") 

```


2. Local Moran's I

```{r}

## used to generate the weights of neighborhoods
## prerequisite for Local Moran
final_net3.nb <- poly2nb(as_Spatial(final_net3), queen=TRUE) # identify neighborhood units
final_net3.weights <- nb2listw(final_net3.nb, style="W", zero.policy=TRUE) 
print(final_net3.weights, zero.policy=TRUE)


# Local Moran
# calculates the local Moran's I statistic for the Police Stations variable within the final_net data frame
local_morans <- localmoran(final_net3$PolStat, final_net3.weights, zero.policy=TRUE) %>% 
  as.data.frame()

# join local Moran's I results to fishnet
final_net3.localMorans <- 
  cbind(local_morans, as.data.frame(final_net3)) %>% 
  st_sf() %>%
  dplyr::select(polCount = Pol_Station, 
                Local_Morans_I = Ii, 
                P_Value = `Pr(z != E(Ii))`) %>%
  mutate(Significant_Hotspots = ifelse(P_Value <= 0.001, 1, 0)) %>%
  gather(Variable, Value, -geometry)
  
```


3.  Visualize local Moran's I results

*Identify Hot Spots Area*
using Local Moran's I, P value to identify the significance
Significant_Hotspots more directly shows the hot spots area, where abandoned cars concentrated.

```{r fig.width=10, fig.height=4}

vars <- unique(final_net3.localMorans$Variable)
varList <- list()

for(i in vars){
  varList[[i]] <- 
    ggplot() +
      geom_sf(data = filter(final_net3.localMorans, Variable == i), 
              aes(fill = Value), colour=NA) +
      scale_fill_viridis(name="") +
      labs(title=i) +
      theme_void() + theme(legend.position="bottom")}

do.call(grid.arrange,c(varList, ncol = 4, top = "Local Morans I statistics, Police Station"))
```


4. Distance to Hot spot

```{r}
# generates warning from NN
final_net3 <- final_net3 %>% 
  mutate(Station.isSig = 
           ifelse(local_morans[,5] <= 0.001, 1, 0)) %>% # identify hotspot area
  mutate(Station.isSig.dist = 
           nn_function(st_c(st_coid(final_net3)),
                       st_c(st_coid(filter(final_net3, 
                                           Station.isSig == 1))), # dist to the hotspot
                       k = 3))


ggplot() +
      geom_sf(data = final_net3, aes(fill=Station.isSig.dist), colour=NA) +
      scale_fill_viridis(name="NN Distance") +
      labs(title="Police Station NN Distance") +
      theme_void()
```

## Combine with neighborhoods and policing districts

```{r}
final_net3 <-
  st_centroid(final_net3) %>%
    st_join(dplyr::select(neighborhoods, name), by = "uniqueID") %>%
    st_join(dplyr::select(policeDistricts, District), by = "uniqueID") %>%
      st_drop_geometry() %>%
      left_join(dplyr::select(final_net3, geometry, uniqueID)) %>%
      st_sf() %>%
  na.omit()
```

## Variables Correlation

```{r}

final_net3_long <- pivot_longer(
  data = final_net3,
  cols = c("Abandoned_Cars.nn", "abandoned.isSig.dist", "Sanitation.nn", "Sani.isSig.dist", "PolStation.nn", "Station.isSig.dist"),
  names_to = "Variable",
  values_to = "Value"
)

# Create the plot
ggplot(final_net3_long, aes(x = Value, y = countRob)) +
  geom_point(size = 0.3) +
  geom_smooth(method = "lm", se = FALSE, color = "indianred2") +
  facet_wrap(~Variable, ncol = 3, scales = "free") +
  labs(title = "Correlation with countRob") +
  theme_minimal()

```

## Histogram of dependent variables (Robbery count)

```{r}
ggplot(final_net3, aes(x = countRob)) + 
  geom_histogram(binwidth = 1, fill = "indianred2", color = "white") +
  labs(title = "Histogram of countRob",
       x = "countRob",
       y = "Frequency") +
  theme_minimal()
```


## Modeling and CV

Leave One Group Out CV on spatial features
can view the "crossValidation" function first

```{r results='hide'}

## define the variables we want
reg.vars <- c("Abandoned_Cars.nn", "Sanitation.nn", "PolStation.nn")

reg.ss.vars <- c("Abandoned_Cars.nn", "abandoned.isSig.dist", "Sanitation.nn", "Sani.isSig.dist", "PolStation.nn", "Station.isSig.dist")

reg.cv <- crossValidate(
  dataset = final_net3,
  id = "cvID",
  dependentVariable = "countRob",
  indVariables = reg.vars) %>%
    dplyr::select(cvID = cvID, countRob, Prediction, geometry)

reg.ss.cv <- crossValidate(
  dataset = final_net3,
  id = "cvID",
  dependentVariable = "countRob",
  indVariables = reg.ss.vars) %>%
    dplyr::select(cvID = cvID, countRob, Prediction, geometry)
  
reg.spatialCV <- crossValidate(
  dataset = final_net3,
  id = "name",
  dependentVariable = "countRob",
  indVariables = reg.vars) %>%
    dplyr::select(cvID = name, countRob, Prediction, geometry)

reg.ss.spatialCV <- crossValidate(
  dataset = final_net3,
  id = "name",
  dependentVariable = "countRob",
  indVariables = reg.ss.vars) %>%
    dplyr::select(cvID = name, countRob, Prediction, geometry)

```


## Cross Validation & K Fold MAE Comparison

```{r fig.width=10, fig.height=4}


reg.summary <- 
  rbind(
    mutate(reg.cv,           Error = Prediction - countRob,
                             Regression = "Random k-fold CV: Just Risk Factors"),
                             
    mutate(reg.ss.cv,        Error = Prediction - countRob,
                             Regression = "Random k-fold CV: Spatial Process"),
    
    mutate(reg.spatialCV,    Error = Prediction - countRob,
                             Regression = "Spatial LOGO-CV: Just Risk Factors"),
                             
    mutate(reg.ss.spatialCV, Error = Prediction - countRob,
                             Regression = "Spatial LOGO-CV: Spatial Process")) %>%
    st_sf()
  
error_by_reg_and_fold <- 
  reg.summary %>%
    group_by(Regression, cvID) %>% 
    summarize(Mean_Error = mean(Prediction - countRob, na.rm = T),
              MAE = mean(abs(Mean_Error), na.rm = T),
              SD_MAE = mean(abs(Mean_Error), na.rm = T)) %>%
  ungroup()

error_by_reg_and_fold %>%
  ggplot(aes(MAE)) + 
    geom_histogram(bins = 30, colour="white", fill = "indianred2") +
    facet_wrap(~Regression) +  
    geom_vline(xintercept = 0) + scale_x_continuous(breaks = seq(0, 8, by = 1)) + 
    labs(title="Distribution of MAE", subtitle = "k-fold cross validation vs. LOGO-CV",
         x="Mean Absolute Error", y="Count") +
    plotTheme()
```
```{r}
st_drop_geometry(error_by_reg_and_fold) %>%
  group_by(Regression) %>% 
    summarize(Mean_MAE = round(mean(MAE), 2),
              SD_MAE = round(sd(MAE), 2)) %>%
  kable() %>%
    kable_styling("striped", full_width = F) %>%
    row_spec(2, color = "black", background = "lightgrey") %>%
    row_spec(4, color = "black", background = "lightgrey") 
```

## Racial Context

A table of raw errors by race context for a random k-fold vs. spatial cross validation regression.

```{r Racial}

tracts18 <- 
  get_acs(geography = "tract", variables = c("B01001_001E","B01001A_001E"), 
          year = 2018, state=17, county=031, geometry=T) %>%
  st_transform('ESRI:102271')  %>% 
  dplyr::select(variable, estimate, GEOID) %>%
  spread(variable, estimate) %>%
  rename(TotalPop = B01001_001,
         NumberWhites = B01001A_001) %>%
  mutate(percentWhite = NumberWhites / TotalPop,
         raceContext = ifelse(percentWhite > .5, "Majority_White", "Majority_Non_White")) %>%
  .[neighborhoods,]

reg.summary %>% 
  filter(str_detect(Regression, "LOGO")) %>%
    st_centroid() %>%
    st_join(tracts18) %>%
    na.omit() %>%
      st_drop_geometry() %>%
      group_by(Regression, raceContext) %>%
      summarize(mean.Error = mean(Error, na.rm = T)) %>%
      spread(raceContext, mean.Error) %>%
      kable(caption = "Mean Error by neighborhood racial context") %>%
        kable_styling("striped", full_width = F)  

```

## Distribution of MAE
## MAE by Neighborhood

```{r}
# calculate errors by NEIGHBORHOOD
error_by_reg_and_fold <- 
  reg.ss.spatialCV %>%
    group_by(cvID) %>% 
    summarize(Mean_Error = mean(Prediction - countRob, na.rm = T),
              MAE = mean(abs(Mean_Error), na.rm = T),
              SD_MAE = mean(abs(Mean_Error), na.rm = T)) %>%
  ungroup()

error_by_reg_and_fold %>% 
  arrange(MAE)

## plot histogram of OOF (out of fold) errors
error_by_reg_and_fold %>%
  ggplot(aes(MAE)) + 
    geom_histogram(bins = 30, colour="white", fill = "indianred2") +
  scale_x_continuous(breaks = seq(0, 11, by = 1)) + 
    labs(title="Distribution of MAE", subtitle = "LOGO-CV",
         x="Mean Absolute Error", y="Count") 
```


## Comparison with 2018 Robbery Data

1.Get 2018 crime data

```{r}
Rob18 <- 
  read.socrata("https://data.cityofchicago.org/Public-Safety/Crimes-2018/3i3m-jwuy") %>% 
  filter(Primary.Type == "ROBBERY" & Description == "ARMED: HANDGUN") %>%
  mutate(x = gsub("[()]", "", Location)) %>%
  separate(x,into= c("Y","X"), sep=",") %>%
  mutate(X = as.numeric(X),
         Y = as.numeric(Y)) %>% 
  na.omit %>%
  st_as_sf(coords = c("X", "Y"), crs = 4326, agr = "constant") %>%
  st_transform('ESRI:102271') %>% 
  distinct() %>%
  .[fishnet,]

burg_ppp <- as.ppp(st_coordinates(Rob17), W = st_bbox(final_net3))
burg_KD.1000 <- density.ppp(burg_ppp, 1000)

burg_KDE_sum <- as.data.frame(burg_KD.1000) %>%
  st_as_sf(coords = c("x", "y"), crs = st_crs(final_net3)) %>%
  aggregate(., final_net3, mean) 
kde_breaks <- classIntervals(burg_KDE_sum$value, 
                             n = 5, "fisher")

##put into discrete groups
burg_KDE_sf <- burg_KDE_sum %>%
  mutate(label = "Kernel Density",
         Risk_Category = classInt::findCols(kde_breaks),
         Risk_Category = case_when(
           Risk_Category == 5 ~ "5th",
           Risk_Category == 4 ~ "4th",
           Risk_Category == 3 ~ "3rd",
           Risk_Category == 2 ~ "2nd",
           Risk_Category == 1 ~ "1st")) %>%
  cbind(
    aggregate(
      dplyr::select(Rob18) %>% mutate(robCount = 1), ., sum) %>%
    mutate(robCount = replace_na(robCount, 0))) %>%
  dplyr::select(label, Risk_Category, robCount)
```



```{r}
ml_breaks <- classIntervals(reg.ss.spatialCV$Prediction, 
                             n = 5, "fisher")
burg_risk_sf <-
  reg.ss.spatialCV %>%
  mutate(label = "Risk Predictions",
         Risk_Category =classInt::findCols(ml_breaks),
         Risk_Category = case_when(
           Risk_Category == 5 ~ "5th",
           Risk_Category == 4 ~ "4th",
           Risk_Category == 3 ~ "3rd",
           Risk_Category == 2 ~ "2nd",
           Risk_Category == 1 ~ "1st")) %>%
  cbind(
    aggregate(
      dplyr::select(Rob18) %>% mutate(robCount = 1), ., sum) %>%
      mutate(robCount = replace_na(robCount, 0))) %>%
  dplyr::select(label,Risk_Category, robCount)

rbind(burg_KDE_sf, burg_risk_sf) %>%
  na.omit() %>%
  gather(Variable, Value, -label, -Risk_Category, -geometry) %>%
  ggplot() +
    geom_sf(aes(fill = Risk_Category), colour = NA) +
    geom_sf(data = sample_n(Rob18, 3000), size = .1, colour = "white") +
    facet_wrap(~label, ) +
    scale_fill_viridis(discrete = TRUE) +
    labs(title="Comparison of Kernel Density and Risk Predictions",
         subtitle="2017 Robberies risk predictions; 2018 Robberies") +
    mapTheme(title_size = 14)

rbind(burg_KDE_sf, burg_risk_sf) %>%
  st_drop_geometry() %>%
  na.omit() %>%
  gather(Variable, Value, -label, -Risk_Category) %>%
  group_by(label, Risk_Category) %>%
  summarize(countRob = sum(Value)) %>%
  ungroup() %>%
  group_by(label) %>%
  mutate(Pcnt_of_test_set_crimes = countRob / sum(countRob)) %>%
    ggplot(aes(Risk_Category,Pcnt_of_test_set_crimes)) +
      geom_bar(aes(fill=label), position="dodge", stat="identity") +
      scale_fill_viridis(discrete = TRUE, name = "Model") +
      labs(title = "Risk prediction vs. Kernel density, 2018 Robberies",
           y = "% of Test Set Robberies (per model)",
           x = "Risk Category") +
  theme_bw() +
      theme(axis.text.x = element_text(angle = 45, vjust = 0.5))
```

1st Risk Category: There's a notable discrepancy in the first risk category, with the prediction model significantly underestimating the percentage of robberies that fall into the highest risk category. The spatial map confirms that the model failed to capture the concentration of the highest risk areas.

2nd and 3rd Risk Categories: For the second and third risk categories, the prediction model seems to overestimate the percentage of robberies when compared to the kernel density. This suggests that the model may misclassify some areas into a higher risk category than they actually are. The spatial distribution might show a broader spread of moderate risk than what actually occurred.

4th Risk Category: The predicted and actual data are closely aligned in the fourth risk category, indicating the model's accuracy for lower-middle-risk areas.

5th Risk Category: There's an underestimation in the prediction for the lowest risk category. This underestimation, combined with the overestimation in the second and third categories, may imply that the model is biased towards predicting a higher risk than exists.

The model is conservative in predicting the highest risk areas, which might cause insufficient resource allocation to the most vulnerable spots.There is an over-prediction in the mid-risk categories, which could lead to an over-allocation of resources to areas that do not require as intensive monitoring or intervention.The accuracy in the fourth category suggests that the model has some predictive power and is not uniformly over or underestimating across all categories. The under-prediction in the lowest risk category might not be as critical from a resource allocation perspective, but it still indicates that the model’s threshold for risk is skewed.

The predictive model’s underestimation of the highest risk robbery areas and overestimation of mid-level risks, as shown in the comparison between prediction and kernel density, suggests it's not yet suited for deployment. The spatial maps underscore the urgency for recalibration to accurately target resources, avoid potential biases, and ensure public safety without misdirecting law enforcement efforts. Before considering production use, the model demands rigorous refinement, transparent methodology, and ethical oversight, coupled with an ongoing validation process that adapts to evolving crime patterns and incorporates a broader spectrum of data for enhanced accuracy.
